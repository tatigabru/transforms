{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate ctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-655650cfbc80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;31m# progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m \u001b[1;31m# library for audio processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import IPython.display\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "# Packages we're using\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# signal processing\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, lfilter, deconvolve\n",
    "import scipy.ndimage\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "import librosa # library for audio processing\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the Spectrograms and Inversion are taken from: https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype=\"band\")\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def overlap(X, window_size, window_step):\n",
    "    \"\"\"\n",
    "    Create an overlapped version of X\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape=(n_samples,)\n",
    "        Input signal to window and overlap\n",
    "    window_size : int\n",
    "        Size of windows to take\n",
    "    window_step : int\n",
    "        Step size between windows\n",
    "    Returns\n",
    "    -------\n",
    "    X_strided : shape=(n_windows, window_size)\n",
    "        2D array of overlapped X\n",
    "    \"\"\"\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be even!\")\n",
    "    # Make sure there are an even number of windows before stridetricks\n",
    "    append = np.zeros((window_size - len(X) % window_size))\n",
    "    X = np.hstack((X, append))\n",
    "\n",
    "    ws = window_size\n",
    "    ss = window_step\n",
    "    a = X\n",
    "\n",
    "    valid = len(a) - ws\n",
    "    nw = (valid) // ss\n",
    "    out = np.ndarray((nw, ws), dtype=a.dtype)\n",
    "\n",
    "    for i in np.arange(nw):\n",
    "        # \"slide\" the window along the samples\n",
    "        start = i * ss\n",
    "        stop = start + ws\n",
    "        out[i] = a[start:stop]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def stft(\n",
    "    X, fftsize=128, step=65, mean_normalize=True, real=False, compute_onesided=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute STFT for 1D real valued input X\n",
    "    \"\"\"\n",
    "    if real:\n",
    "        local_fft = np.fft.rfft\n",
    "        cut = -1\n",
    "    else:\n",
    "        local_fft = np.fft.fft\n",
    "        cut = None\n",
    "    if compute_onesided:\n",
    "        cut = fftsize // 2\n",
    "    if mean_normalize:\n",
    "        X -= X.mean()\n",
    "\n",
    "    X = overlap(X, fftsize, step)\n",
    "\n",
    "    size = fftsize\n",
    "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "    X = X * win[None]\n",
    "    X = local_fft(X)[:, :cut]\n",
    "    return X\n",
    "\n",
    "\n",
    "def pretty_spectrogram(d, log=True, thresh=5, fft_size=512, step_size=64):\n",
    "    \"\"\"\n",
    "    creates a spectrogram\n",
    "    log: take the log of the spectrgram\n",
    "    thresh: threshold minimum power for log spectrogram\n",
    "    \"\"\"\n",
    "    specgram = np.abs(\n",
    "        stft(d, fftsize=fft_size, step=step_size, real=False, compute_onesided=True)\n",
    "    )\n",
    "\n",
    "    if log == True:\n",
    "        specgram /= specgram.max()  # volume normalize to max 1\n",
    "        specgram = np.log10(specgram)  # take log\n",
    "        specgram[\n",
    "            specgram < -thresh\n",
    "        ] = -thresh  # set anything less than the threshold as the threshold\n",
    "    else:\n",
    "        specgram[\n",
    "            specgram < thresh\n",
    "        ] = thresh  # set anything less than the threshold as the threshold\n",
    "\n",
    "    return specgram\n",
    "\n",
    "\n",
    "# Also mostly modified or taken from https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "def invert_pretty_spectrogram(\n",
    "    X_s, log=True, fft_size=512, step_size=512 / 4, n_iter=10\n",
    "):\n",
    "\n",
    "    if log == True:\n",
    "        X_s = np.power(10, X_s)\n",
    "\n",
    "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
    "    X_t = iterate_invert_spectrogram(X_s, fft_size, step_size, n_iter=n_iter)\n",
    "    return X_t\n",
    "\n",
    "\n",
    "def iterate_invert_spectrogram(X_s, fftsize, step, n_iter=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    reg = np.max(X_s) / 1e8\n",
    "    X_best = copy.deepcopy(X_s)\n",
    "    for i in range(n_iter):\n",
    "        if verbose:\n",
    "            print(\"Runnning iter %i\" % i)\n",
    "        if i == 0:\n",
    "            X_t = invert_spectrogram(\n",
    "                X_best, step, calculate_offset=True, set_zero_phase=True\n",
    "            )\n",
    "        else:\n",
    "            # Calculate offset was False in the MATLAB version\n",
    "            # but in mine it massively improves the result\n",
    "            # Possible bug in my impl?\n",
    "            X_t = invert_spectrogram(\n",
    "                X_best, step, calculate_offset=True, set_zero_phase=False\n",
    "            )\n",
    "        est = stft(X_t, fftsize=fftsize, step=step, compute_onesided=False)\n",
    "        phase = est / np.maximum(reg, np.abs(est))\n",
    "        X_best = X_s * phase[: len(X_s)]\n",
    "    X_t = invert_spectrogram(X_best, step, calculate_offset=True, set_zero_phase=False)\n",
    "    return np.real(X_t)\n",
    "\n",
    "\n",
    "def invert_spectrogram(X_s, step, calculate_offset=True, set_zero_phase=True):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    size = int(X_s.shape[1] // 2)\n",
    "    wave = np.zeros((X_s.shape[0] * step + size))\n",
    "    # Getting overflow warnings with 32 bit...\n",
    "    wave = wave.astype(\"float64\")\n",
    "    total_windowing_sum = np.zeros((X_s.shape[0] * step + size))\n",
    "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "\n",
    "    est_start = int(size // 2) - 1\n",
    "    est_end = est_start + size\n",
    "    for i in range(X_s.shape[0]):\n",
    "        wave_start = int(step * i)\n",
    "        wave_end = wave_start + size\n",
    "        if set_zero_phase:\n",
    "            spectral_slice = X_s[i].real + 0j\n",
    "        else:\n",
    "            # already complex\n",
    "            spectral_slice = X_s[i]\n",
    "\n",
    "        # Don't need fftshift due to different impl.\n",
    "        wave_est = np.real(np.fft.ifft(spectral_slice))[::-1]\n",
    "        if calculate_offset and i > 0:\n",
    "            offset_size = size - step\n",
    "            if offset_size <= 0:\n",
    "                print(\n",
    "                    \"WARNING: Large step size >50\\% detected! \"\n",
    "                    \"This code works best with high overlap - try \"\n",
    "                    \"with 75% or greater\"\n",
    "                )\n",
    "                offset_size = step\n",
    "            offset = xcorr_offset(\n",
    "                wave[wave_start : wave_start + offset_size],\n",
    "                wave_est[est_start : est_start + offset_size],\n",
    "            )\n",
    "        else:\n",
    "            offset = 0\n",
    "        wave[wave_start:wave_end] += (\n",
    "            win * wave_est[est_start - offset : est_end - offset]\n",
    "        )\n",
    "        total_windowing_sum[wave_start:wave_end] += win\n",
    "    wave = np.real(wave) / (total_windowing_sum + 1e-6)\n",
    "    return wave\n",
    "\n",
    "\n",
    "def xcorr_offset(x1, x2):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    x1 = x1 - x1.mean()\n",
    "    x2 = x2 - x2.mean()\n",
    "    frame_size = len(x2)\n",
    "    half = frame_size // 2\n",
    "    corrs = np.convolve(x1.astype(\"float32\"), x2[::-1].astype(\"float32\"))\n",
    "    corrs[:half] = -1e30\n",
    "    corrs[-half:] = -1e30\n",
    "    offset = corrs.argmax() - len(x1)\n",
    "    return offset\n",
    "\n",
    "\n",
    "def make_mel(spectrogram, mel_filter, shorten_factor=1):\n",
    "    mel_spec = np.transpose(mel_filter).dot(np.transpose(spectrogram))\n",
    "    mel_spec = scipy.ndimage.zoom(\n",
    "        mel_spec.astype(\"float32\"), [1, 1.0 / shorten_factor]\n",
    "    ).astype(\"float16\")\n",
    "    mel_spec = mel_spec[:, 1:-1]  # a little hacky but seemingly needed for clipping\n",
    "    return mel_spec\n",
    "\n",
    "\n",
    "def mel_to_spectrogram(mel_spec, mel_inversion_filter, spec_thresh, shorten_factor):\n",
    "    \"\"\"\n",
    "    takes in an mel spectrogram and returns a normal spectrogram for inversion \n",
    "    \"\"\"\n",
    "    mel_spec = mel_spec + spec_thresh\n",
    "    uncompressed_spec = np.transpose(np.transpose(mel_spec).dot(mel_inversion_filter))\n",
    "    uncompressed_spec = scipy.ndimage.zoom(\n",
    "        uncompressed_spec.astype(\"float32\"), [1, shorten_factor]\n",
    "    ).astype(\"float16\")\n",
    "    uncompressed_spec = uncompressed_spec - 4\n",
    "    return uncompressed_spec\n",
    "\n",
    "\n",
    "# From https://github.com/jameslyons/python_speech_features\n",
    "\n",
    "\n",
    "def hz2mel(hz):\n",
    "    \"\"\"Convert a value in Hertz to Mels\n",
    "    :param hz: a value in Hz. This can also be a numpy array, conversion proceeds element-wise.\n",
    "    :returns: a value in Mels. If an array was passed in, an identical sized array is returned.\n",
    "    \"\"\"\n",
    "    return 2595 * np.log10(1 + hz / 700.0)\n",
    "\n",
    "\n",
    "def mel2hz(mel):\n",
    "    \"\"\"Convert a value in Mels to Hertz\n",
    "    :param mel: a value in Mels. This can also be a numpy array, conversion proceeds element-wise.\n",
    "    :returns: a value in Hertz. If an array was passed in, an identical sized array is returned.\n",
    "    \"\"\"\n",
    "    return 700 * (10 ** (mel / 2595.0) - 1)\n",
    "\n",
    "\n",
    "def get_filterbanks(nfilt=20, nfft=512, samplerate=16000, lowfreq=0, highfreq=None):\n",
    "    \"\"\"Compute a Mel-filterbank. The filters are stored in the rows, the columns correspond\n",
    "    to fft bins. The filters are returned as an array of size nfilt * (nfft/2 + 1)\n",
    "    :param nfilt: the number of filters in the filterbank, default 20.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param samplerate: the samplerate of the signal we are working with. Affects mel spacing.\n",
    "    :param lowfreq: lowest band edge of mel filters, default 0 Hz\n",
    "    :param highfreq: highest band edge of mel filters, default samplerate/2\n",
    "    :returns: A numpy array of size nfilt * (nfft/2 + 1) containing filterbank. Each row holds 1 filter.\n",
    "    \"\"\"\n",
    "    highfreq = highfreq or samplerate / 2\n",
    "    assert highfreq <= samplerate / 2, \"highfreq is greater than samplerate/2\"\n",
    "\n",
    "    # compute points evenly spaced in mels\n",
    "    lowmel = hz2mel(lowfreq)\n",
    "    highmel = hz2mel(highfreq)\n",
    "    melpoints = np.linspace(lowmel, highmel, nfilt + 2)\n",
    "    # our points are in Hz, but we use fft bins, so we have to convert\n",
    "    #  from Hz to fft bin number\n",
    "    bin = np.floor((nfft + 1) * mel2hz(melpoints) / samplerate)\n",
    "\n",
    "    fbank = np.zeros([nfilt, nfft // 2])\n",
    "    for j in range(0, nfilt):\n",
    "        for i in range(int(bin[j]), int(bin[j + 1])):\n",
    "            fbank[j, i] = (i - bin[j]) / (bin[j + 1] - bin[j])\n",
    "        for i in range(int(bin[j + 1]), int(bin[j + 2])):\n",
    "            fbank[j, i] = (bin[j + 2] - i) / (bin[j + 2] - bin[j + 1])\n",
    "    return fbank\n",
    "\n",
    "\n",
    "def create_mel_filter(\n",
    "    fft_size, n_freq_components=64, start_freq=300, end_freq=8000, samplerate=44100\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a filter to convolve with the spectrogram to get out mels\n",
    "\n",
    "    \"\"\"\n",
    "    mel_inversion_filter = get_filterbanks(\n",
    "        nfilt=n_freq_components,\n",
    "        nfft=fft_size,\n",
    "        samplerate=samplerate,\n",
    "        lowfreq=start_freq,\n",
    "        highfreq=end_freq,\n",
    "    )\n",
    "    # Normalize filter\n",
    "    mel_filter = mel_inversion_filter.T / mel_inversion_filter.sum(axis=1)\n",
    "\n",
    "    return mel_filter, mel_inversion_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "fft_size = 2048  # window size for the FFT\n",
    "step_size = fft_size // 16  # distance to slide along the window (in time)\n",
    "spec_thresh = 4  # threshold for spectrograms (lower filters out more noise)\n",
    "lowcut = 500  # Hz # Low cut for our butter bandpass filter\n",
    "highcut = 15000  # Hz # High cut for our butter bandpass filter\n",
    "# For mels\n",
    "n_mel_freq_components = 64  # number of mel frequency channels\n",
    "shorten_factor = 10  # how much should we compress the x-axis (time)\n",
    "start_freq = 300  # Hz # What frequency to start sampling our melS from\n",
    "end_freq = 8000  # Hz # What frequency to stop sampling our melS from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_spectrogram = pretty_spectrogram(\n",
    "    data.astype(\"float64\"),\n",
    "    fft_size=fft_size,\n",
    "    step_size=step_size,\n",
    "    log=True,\n",
    "    thresh=spec_thresh,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
